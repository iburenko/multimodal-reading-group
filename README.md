# multimodal-reading-group

---
| Date | Paper | Authors | Comments |
| :--- | :---- | :------ | :------- |
| 01.02.2024 | [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) | H. Liu, C. Li, Q. Wu, Y. J. Lee ||
| 08.02.2024 | [When and why vision-language models behave like bags-of-words, and what to do about it?](https://arxiv.org/abs/2210.01936) | M. Yuksekgonul, F. Bianchi, P. Kalluri, D. Jurafsky, J. Zou | Why did they expect that CLIP will take a word order into account given that CLIP is trained to match a bag-of-words with a corresponding image? |
| 22.02.2024 | [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) | A. Radford, J.W. Kim, C. Hallacy, A. Ramesh, G.Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, I. Sutskever | See also [open source implementation](https://github.com/mlfoundations/open_clip) of CLIP; [Scaling laws for contrastive language-image learning](https://arxiv.org/abs/2212.07143)| 
| 29.02.2024 | Continue | | Fig. 2 is unclear. How do they obtain a vector for a bag-of-words? | 
| 07.03.2024 | Still (sic!) continue | | It seems that they train using BoW, even though their inference pipeline does not reflect this. | 
| ??.03.2024 | [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/abs/2303.15343) | X. Zhai, B. Mustafa, A. Kolesnikov, L. Beyer ||
---
